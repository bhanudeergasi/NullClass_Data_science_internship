{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhanudeergasi/NullClass_Data_science_internship/blob/main/text_encoder_task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oem-aP6bhVWN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqD04FnPhrnY"
      },
      "source": [
        "#  Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1REngDChlrK",
        "outputId": "60460e94-7f0a-4f15-9491-bc277fea7519"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EJYrY_5hukP"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5hWKQ7FhmFg"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPTokenizer, CLIPTextModel, CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGA2TdIohzjE"
      },
      "source": [
        "Mount to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaMgyrkXhmUR",
        "outputId": "ba69e7f1-a6f1-4580-a462-36a4cfd47702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sopJs-U3hml-"
      },
      "outputs": [],
      "source": [
        "project_path = \"/content/drive/MyDrive/TASK3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1xAjzJth_Wi"
      },
      "source": [
        "#  Load Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUcgqll1h-Re"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(project_path, \"promts.json\")) as f:\n",
        "    text_prompts = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "103z8RHaiFik"
      },
      "source": [
        "#  Load CLIP Text Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "85c5e3f70f0941708290fe57f306aa6a",
            "f9bef11c49004d03891fcf0cdcf5e258",
            "6fe44d6f8e27416799e9d4400d62b00b",
            "215f858470df4c5ebf6c24fb32f78378",
            "d63c5f2069834d68b3c78ac29436b80d",
            "2b4bf6be455d4fd3bfcfa8c3576736c2",
            "124e7a5d426c4e00bed47aae0293e4e3",
            "7ee2702e228141b29932aec650245f74",
            "614338170b224a298f6ccae58c6e780d",
            "9d8a2782b13149c79575d963aa2bf4fa",
            "83c1e033a47e425c9dbb0ce169a37375",
            "c8ef24aab732491fa6f18f76c905b197",
            "13617b5abc894c0f9aaf08e29335b06c",
            "2d06a9eb6539455ab58bbf9c50829242",
            "6bf2a67e2f544951b270fd517868163b",
            "2a1cbea16f7f4753a1e84f74e5092d64",
            "1e69b3b37ea24caaaf101526cebfb0a7",
            "a7bb37d937194f89a128a92f16b96dbc",
            "55900cd346a84c2fae87b8648b5a20ea",
            "7e2a67e10078419483b93f1554173fe6",
            "b1cacbaff6fb4b6a84bdb055a259fb45",
            "cb1d134f91c64e9a80cbec0f95e312be",
            "266c2585a1164956a929ac25a99f4323",
            "0002c6910aad4e6791bc8f656828eac6",
            "01e0cb9ab0074713a1c82d82c1fffc13",
            "692376d0d23c438a819ed092d18e54ff",
            "4538863c83d64b7699a14cac1777fb20",
            "d0e43eed39424ac0b9a0ab5ef04df86a",
            "110b9f68205d420f985c72c38697e104",
            "47c825de571844f381fb21b2628626e9",
            "4d75d7bee17d474685ee12e25ec22db0",
            "4577f8055f0c4ad0b33c87595916d1d4",
            "f0617b9a178c4166bfbbb4c08f4747a3",
            "a454c3520169445fb5f49620842af8e9",
            "0b0fe7dbea474cfcb521cb6c392d7fbc",
            "b824575041e4437bbe587b2309488986",
            "93d97ae51c0442cf82ff111144ba89d7",
            "38e7d4f0c3d94dc192611ff291f89b88",
            "32751fc3cd854c00ab2fd6571b2b4ede",
            "3be084d136ed4597bf56eeb3da789f34",
            "bb6fedb63b984cea9ed4f10b0b0a9ddc",
            "4d67255d5a5d4b0d9503519ea3305c1d",
            "09b35bfe511b4e059bb9dcfbb1a68911",
            "8c02bfaa1463480eb2af7b045a09bdb7",
            "d14560c800ea49c8900947ed5b0f0286",
            "7686131b702f40ca9cc849cc8b8912d1",
            "2b2067e1b59e46ad93b494a1c2a6ea49",
            "59cd1caa5891488b8705c72fe697c1e3",
            "6b30b0a3858e43808cd9fcaae4ee9e79",
            "773b39bf75d342c58edb58f755c72386",
            "cbd977ae206b4e3fbfde98b7958d0067",
            "423a947c1d394e4b9663ad7955e9dfd4",
            "d609701d12d14816b5b82c1fb3f8c0ba",
            "b06e8ffe4956490b90a27abdb4c143f8",
            "fbb81eb7af63492db77acdd9a41c550d",
            "cc6071c64dde41418290a03ad45c13e3",
            "7c6ee2610f5b4a7baac2c917f2ddc092",
            "5c26e9b85b804c2597c18e1a78a0f259",
            "4d55a149d16645659532a41264a9ba79",
            "47486a4311d44ff7bae6a7455c01eead",
            "828c8280a9b149a891289f095af04c31",
            "9aa5903445d747068e70544a8fb5e83c",
            "6c215f2049044289a36ca5cf0a75b78e",
            "78961cf1ff614240be4257ffcd8b232c",
            "be47866c03584f4fa911d0613ef5909c",
            "a4efd47ebe6b42b286ef3529fe1d09ad",
            "81ebc7a74ded4828857cdc562360aa1a",
            "e5703e77171446ea933bd56c3c1eef0b",
            "646c8b83c2344e4c82ba1dbc188774f4",
            "c761bb181b6747588ec390de27cbcccc",
            "397988938f264ba6b4f5744e2ed76d5a",
            "5cca8400fce647b18d651c89d30582fa",
            "48a26110a4a8427c93e0e7f3e706a8cb",
            "df2486e73d424724bbd2950744947d35",
            "24d3e6b8259a4f3dab99a7004113bc6a",
            "8aaf7339564c4e4194356efd6d3bc5e2",
            "81463027c9574abd8c3d9898b4ffebb8"
          ]
        },
        "id": "EK0JSc0kh-nC",
        "outputId": "f7274d3e-4009-4f24-850c-6e23d689fb98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85c5e3f70f0941708290fe57f306aa6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8ef24aab732491fa6f18f76c905b197",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "266c2585a1164956a929ac25a99f4323",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a454c3520169445fb5f49620842af8e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d14560c800ea49c8900947ed5b0f0286",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc6071c64dde41418290a03ad45c13e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81ebc7a74ded4828857cdc562360aa1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTSFLHrCiJ6o"
      },
      "source": [
        "#  Generate Text Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "98c9aa40f6ff4fadb3cf74324d52cc6d",
            "b8a0f703fe3149a9a5d10eb3cc10567b",
            "6d72a849cd0947929f16ba681c55d392",
            "1868733860de4f7c911e6ad713ec0788",
            "020e73243601429590d0a52090c572c5",
            "63b8c6c912864847b35d54c0c0179e34",
            "87c631f73a39462b8256e45a469d4919",
            "b6cb8a4acb694908be83e8a9fd2af8f0",
            "109a82f3932a420fab4f097636ad40e5",
            "86a1d0a24bb9442398f854ff67cd894f",
            "6e5132405ffc4b64a692bb146d43220d"
          ]
        },
        "id": "guYiJgl_iIvC",
        "outputId": "3e6da509-ca72-4258-e875-4c1bc51c27b5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98c9aa40f6ff4fadb3cf74324d52cc6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Text embeddings saved to text_embeddings.pt\n"
          ]
        }
      ],
      "source": [
        "all_embeddings = []\n",
        "batch_size = 16\n",
        "\n",
        "for i in range(0, len(text_prompts), batch_size):\n",
        "    batch = text_prompts[i:i + batch_size]\n",
        "    inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = text_model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    all_embeddings.append(embeddings)\n",
        "\n",
        "final_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "torch.save(final_embeddings, os.path.join(project_path, \"text_embeddings.pt\"))\n",
        "print(\" Text embeddings saved to text_embeddings.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "r6-xX46viJCl",
        "outputId": "be5de147-b8e4-4744-bb30-6ec41861411b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Dummy images generated for testing CLIP score.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from PIL import Image, ImageDraw\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "project_path = \"/content/drive/MyDrive/TASK3\"\n",
        "image_output_path = os.path.join(project_path, \"generated_images\")\n",
        "os.makedirs(image_output_path, exist_ok=True)\n",
        "\n",
        "# Generate Dummy Images\n",
        "for i, prompt in enumerate(text_prompts[:10]):\n",
        "    img = Image.new(\"RGB\", (512, 512), color=(255, 255, 255))\n",
        "    d = ImageDraw.Draw(img)\n",
        "    d.text((10, 10), prompt[:40], fill=(0, 0, 0))  # Use part of prompt as label\n",
        "    img_path = os.path.join(image_output_path, f\"generated_image_{i}.png\")\n",
        "    img.save(img_path)\n",
        "\n",
        "print(\" Dummy images generated for testing CLIP score.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8dPjwztiU3W"
      },
      "source": [
        "#  CLIP Score Evaluation (using same prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "bc622b1407f64b40a8e79f9f285b22be",
            "a1344ab2005a41a187d22d255016bef0",
            "9a69ef6403d2414c8fd2769bd186d2ca",
            "8b112470537e4c9d8537df43974b16d3",
            "47024d6b68fd46abb26dc612a51d140c",
            "37991e32f9924d569728f9b1896c03a2",
            "fdfda436b546436097f620583a49a261",
            "e009fa30bbeb429caafb01617a1d6e71",
            "659ef9b81c96497aa0df9b1ab265c464",
            "b6a818ced00744e99d829ac3c74f3e08",
            "7be7c956950a4971a23d0d4af9e1c3be"
          ]
        },
        "id": "G-04FaLNiJVi",
        "outputId": "8d8852df-b6eb-4a39-c84d-c11f42cbf0d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Evaluating CLIP score for image-prompt alignment...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc622b1407f64b40a8e79f9f285b22be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” [0] \"A dog running on the beach...\" â†’ CLIP Score: 1.000\n",
            "ğŸ” [1] \"A futuristic cityscape at nigh...\" â†’ CLIP Score: 1.000\n",
            "ğŸ” [2] \"An astronaut riding a horse on...\" â†’ CLIP Score: 1.000\n",
            "ğŸ” [3] \"A steaming cup of coffee on a ...\" â†’ CLIP Score: 1.000\n",
            "ğŸ” [4] \"A lush forest with a waterfall...\" â†’ CLIP Score: 1.000\n",
            "ğŸ” [5] \"A spaceship landing on an alie...\" â†’ CLIP Score: 1.000\n",
            "ğŸ” [6] \"A cat sitting on a pile of boo...\" â†’ CLIP Score: 1.000\n",
            "ğŸ” [7] \"A sunset over the mountains...\" â†’ CLIP Score: 1.000\n",
            "ğŸ” [8] \"A bowl of fresh fruit on a kit...\" â†’ CLIP Score: 1.000\n",
            "ğŸ” [9] \"A robot playing the violin...\" â†’ CLIP Score: 1.000\n",
            "\n",
            " Final Accuracy: 100.00% (CLIP Score > 0.80)\n"
          ]
        }
      ],
      "source": [
        "# Evaluate CLIP Scores\n",
        "print(\"\\n Evaluating CLIP score for image-prompt alignment...\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "clip_scores = []\n",
        "for i, prompt in enumerate(text_prompts[:10]):\n",
        "    image_path = os.path.join(image_output_path, f\"generated_image_{i}.png\")\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\" Skipping image {i} (not found)\")\n",
        "        continue\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = clip_processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "        score = outputs.logits_per_image.softmax(dim=1).item()\n",
        "        clip_scores.append(score)\n",
        "        print(f\"ğŸ” [{i}] \\\"{prompt[:30]}...\\\" â†’ CLIP Score: {score:.3f}\")\n",
        "\n",
        "#  Final Accuracy\n",
        "passed = sum(s > 0.80 for s in clip_scores)\n",
        "accuracy = passed / len(clip_scores) * 100 if clip_scores else 0\n",
        "print(f\"\\n Final Accuracy: {accuracy:.2f}% (CLIP Score > 0.80)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_JtpfTUiaE3"
      },
      "source": [
        "# Final Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3F1gy8diccW",
        "outputId": "28bf75ce-ca0a-46b1-bf33-faf3a0c9ffba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Final Accuracy: 100.00% (CLIP Score > 0.80)\n"
          ]
        }
      ],
      "source": [
        "passed = sum(s > 0.80 for s in clip_scores)\n",
        "accuracy = passed / len(clip_scores) * 100 if clip_scores else 0\n",
        "print(f\"\\n Final Accuracy: {accuracy:.2f}% (CLIP Score > 0.80)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMX2-oVXnNDq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
